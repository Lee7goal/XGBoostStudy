{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n在统计学中，线性回归是一种线性方法\\n用于对因变量y和一个或多个自变量之间的线性关系进行建模\\n当只有一个自变量时，这种回归分析称为一元回归分析；\\n当有两个或两个以上的自变量时，则称这种回归分析为多元回归分析。\\n线性回归主要解决回归问题，即对连续型的数据进行预测，比如预测房价、销售量等。\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "\"\"\"\n",
    "在统计学中，线性回归是一种线性方法\n",
    "用于对因变量y和一个或多个自变量之间的线性关系进行建模\n",
    "当只有一个自变量时，这种回归分析称为一元回归分析；\n",
    "当有两个或两个以上的自变量时，则称这种回归分析为多元回归分析。\n",
    "线性回归主要解决回归问题，即对连续型的数据进行预测，比如预测房价、销售量等。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n例如，预测房屋价格，以y表示要预测的房屋价格\\n以向量x中的元素xj表示房屋的特征（如房屋面积、卧室数量等）。\\n已知的很多房屋数据中，用xi表示第i个房屋的特征，用yi表示其房屋价格。\\n下面通过公式推导，演示如何使用线性回归解决连续型数据预测的问题。\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "\"\"\"\n",
    "线性回归的目标是:\n",
    "    对于输入向量x，预测其目标值y。\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "例如，预测房屋价格，以y表示要预测的房屋价格\n",
    "以向量x中的元素xj表示房屋的特征（如房屋面积、卧室数量等）。\n",
    "已知的很多房屋数据中，用xi表示第i个房屋的特征，用yi表示其房屋价格。\n",
    "下面通过公式推导，演示如何使用线性回归解决连续型数据预测的问题。\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'w = (X^TX)^(-1)*X^Ty'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "# 因为是连续型数据的预测，我们希望找到一个函数，使得yi=h（xi）\n",
    "# yi=h（xi），则可以通过h（x）对未知价格的房屋进行房价预测\n",
    "# 我们采用线性函数表示h（x），即h（x）=ω^Tx。\n",
    "# 则现在的问题就是如何找到最优的w参数\n",
    "# 即通过y = h(x)能够得到的预测值与真实值误差非常小\n",
    "# L(w) = ∑[m, i=1](yi - w^Txi)^2\n",
    "# 该函数即可视为线性回归的损失函数损失函数值越小则w越趋近于理想值\n",
    "# 这个问题的求解方法很多\n",
    "# 例如使用正规方程\n",
    "l = 'w = (X^TX)^(-1)*X^Ty'\n",
    "l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n1.什么是梯度\\n    多元函数对每个参数求偏导，然后将各个参数的偏导数组合成一个向量，该向量称为梯度\\n    对参数y求偏导数\\n    在点(x0,y0）处，\\n    如果沿着梯度方向▽f（x,y）移动，其函数值增加得最快，换句话说，就是能更快地找到最大值。\\n    反之，如果沿着梯度的反方向移动，则函数值下降得最快，更容易找到最小值。\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ],
   "source": [
    "# 梯度下降法\n",
    "\"\"\"\n",
    "1.什么是梯度\n",
    "    多元函数对每个参数求偏导，然后将各个参数的偏导数组合成一个向量，该向量称为梯度\n",
    "    对参数y求偏导数\n",
    "    在点(x0,y0）处，\n",
    "    如果沿着梯度方向▽f（x,y）移动，其函数值增加得最快，换句话说，就是能更快地找到最大值。\n",
    "    反之，如果沿着梯度的反方向移动，则函数值下降得最快，更容易找到最小值。\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n2.什么是梯度下降\\n    那么运用梯度下降一定会找到全局最优解吗？\\n    并不一定，梯度下降法可能只会得到局部最优解，\\n    就像最陡峭的下坡方向不一定能直接通向山脚，可能只到达山峰的低处。\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.什么是梯度下降\n",
    "    那么运用梯度下降一定会找到全局最优解吗？\n",
    "    并不一定，梯度下降法可能只会得到局部最优解，\n",
    "    就像最陡峭的下坡方向不一定能直接通向山脚，可能只到达山峰的低处。\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n3.梯度下降算法过程\\n    首先确定损失函数的梯度，对于当前位置ωj，梯度如下\\n        aj(w)/awj\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "\"\"\"\n",
    "3.梯度下降算法过程\n",
    "    首先确定损失函数的梯度，对于当前位置ωj，梯度如下\n",
    "        aj(w)/awj\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "相关内容等重修相关数学内容后补齐\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}